{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manasi-ganti/long-doc-summarization-gpt/blob/main/lds_sota.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO4fqk0-8j-x",
        "outputId": "e10c4d62-4222-4a4b-bc54-6f6cca8b4b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (5.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install datasets\n",
        "!python3 -m pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zqUjlr9_6s7",
        "outputId": "6ca8587e-9626-486c-ef45-f01de675de11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.7)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.21.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge_score) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->rouge_score) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge_score) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge_score) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install sentencepiece\n",
        "!python3 -m pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "95f4de6581c547fcb76cd7d03bcbd77c",
            "076cbd31ccd04a62ba7d81a82621dbcb",
            "94d9a6ac56fd4241b664b2131d0d7356",
            "be53bb8e1c4d459b8830656aa26fa161",
            "4735f42c2c514d1e8b5329a62a37ece0",
            "57911031e8f54befb043878f75df08e3",
            "203ff9836aab49b3be16f71cf0a1d4b6",
            "cea4c7194e0741f8afc34176635561b8",
            "38b0ffa48bc54990b50cf2ac0eb8f748",
            "ca7d1ff173684294af25f39034643afe",
            "390e6a6e9013491bb1c6ae7f9f4b308c"
          ]
        },
        "id": "VC_tWfYITRH7",
        "outputId": "cea2d5d8-dab5-44fa-91f5-711ad1d37693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95f4de6581c547fcb76cd7d03bcbd77c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset_builder, load_dataset\n",
        "from datasets import get_dataset_config_names\n",
        "from datasets import get_dataset_split_names\n",
        "\n",
        "from tqdm.auto import tqdm  # for our loading bar\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569,
          "referenced_widgets": [
            "678cd0d1d84d4669a0b949e6e8bb6eba",
            "6b1a8a27cced46f195ad6f6516d0ebe7",
            "0a89b639776b4d6285b5c5441c237e43",
            "bed65a339fb043f799924551571a064e",
            "5968322559a54300ac83a0ceb04e19c4",
            "8e251d5a838d4ea3b3713068932984fd",
            "ccce28a72b564c0db2f32c9dcdd7bdb2",
            "9843462224e34aa686e26cc8eb170888",
            "ebc03b1a0fd74171b068a1c7a8e7075e",
            "d4e0d9b57d20450faf8ef9bef67de474",
            "c112d26a472a4cb9a41b1681c050a33a"
          ]
        },
        "id": "ip5wndQkTbR6",
        "outputId": "7e14a1fb-b374-4df3-fafc-cc6fc501072f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['section', 'document']\n",
            "\n",
            " Arxiv dataset for summarization.\n",
            " From paper: A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents\" by A. Cohan et al.\n",
            " See: https://aclanthology.org/N18-2097.pdf \n",
            " See: https://github.com/armancohan/long-summarization\n",
            "\n",
            "{'article': Value(dtype='string', id=None), 'abstract': Value(dtype='string', id=None)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:No config specified, defaulting to: arxiv-summarization/section\n",
            "WARNING:datasets.builder:Found cached dataset arxiv-summarization (/root/.cache/huggingface/datasets/ccdv___arxiv-summarization/section/1.0.0/fa2c9abf4312afb8660ef8e041d576b8e3943ea96ae771bd3cd091b5798e7cc3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "678cd0d1d84d4669a0b949e6e8bb6eba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ccdv___arxiv-summarization/section/1.0.0/fa2c9abf4312afb8660ef8e041d576b8e3943ea96ae771bd3cd091b5798e7cc3/cache-3548f0435a16ce3f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ccdv___arxiv-summarization/section/1.0.0/fa2c9abf4312afb8660ef8e041d576b8e3943ea96ae771bd3cd091b5798e7cc3/cache-5edd3b173b34a530.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ccdv___arxiv-summarization/section/1.0.0/fa2c9abf4312afb8660ef8e041d576b8e3943ea96ae771bd3cd091b5798e7cc3/cache-87aefaff74c55708.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'abstract'],\n",
            "        num_rows: 203037\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['article', 'abstract'],\n",
            "        num_rows: 6436\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'abstract'],\n",
            "        num_rows: 6440\n",
            "    })\n",
            "})\n",
            "{'article': 'the leptonic decays of a charged pseudoscalar meson @xmath7 are processes of the type @xmath8 , where @xmath9 , @xmath10 , or @xmath11 . because no strong interactions are present in the leptonic final state @xmath12 , such decays provide a clean way to probe the complex , strong interactions that bind the quark and antiquark within the initial - state meson . in these decays , strong interaction effects can be parametrized by a single quantity , @xmath13 , the pseudoscalar meson decay constant . \\n the leptonic decay rate can be measured by experiment , and the decay constant can be determined by the equation ( ignoring radiative corrections ) @xmath14 where @xmath15 is the fermi coupling constant , @xmath16 is the cabibbo - kobayashi - maskawa ( ckm ) matrix  @xcite element , @xmath17 is the mass of the meson , and @xmath18 is the mass of the charged lepton . \\n the quantity @xmath13 describes the amplitude for the @xmath19 and @xmath20-quarks within the @xmath21 to have zero separation , a condition necessary for them to annihilate into the virtual @xmath22 boson that produces the @xmath12 pair . \\n the experimental determination of decay constants is one of the most important tests of calculations involving nonperturbative qcd . \\n such calculations have been performed using various models  @xcite or using lattice qcd ( lqcd ) . \\n the latter is now generally considered to be the most reliable way to calculate the quantity . \\n knowledge of decay constants is important for describing several key processes , such as @xmath23 mixing , which depends on @xmath24 , a quantity that is also predicted by lqcd calculations . \\n experimental determination  @xcite of @xmath24 with the leptonic decay of a @xmath25 meson is , however , very limited as the rate is highly suppressed due to the smallness of the magnitude of the relevant ckm matrix element @xmath26 . \\n the charm mesons , @xmath27 and @xmath28 , are better instruments to study the leptonic decays of heavy mesons since these decays are either less ckm suppressed or favored , _ \\n i.e. _ , @xmath29 and @xmath30 are much larger than @xmath31 . \\n thus , the decay constants @xmath32 and @xmath33 determined from charm meson decays can be used to test and validate the necessary lqcd calculations applicable to the @xmath34-meson sector .    among \\n the leptonic decays in the charm - quark sector , @xmath35 decays are more accessible since they are ckm favored . \\n furthermore , the large mass of the @xmath11 lepton removes the helicity suppression that is present in the decays to lighter leptons . \\n the existence of multiple neutrinos in the final state , however , makes measurement of this decay challenging . \\n physics beyond the standard model ( sm ) might also affect leptonic decays of charmed mesons . \\n depending on the non - sm features , the ratio of @xmath36 could be affected  @xcite , as could the ratio  @xcite @xmath37 . \\n any of the individual widths might be increased or decreased . \\n there is an indication of a discrepancy between the experimental determinations  @xcite of @xmath33 and the most recent precision lqcd calculation  @xcite . \\n this disagreement is particularly puzzling since the cleo - c determination  @xcite of @xmath32 agrees well with the lqcd calculation  @xcite of that quantity . some  @xcite conjecture that this discrepancy may be explained by a charged higgs boson or a leptoquark .    in this article \\n , we report an improved measurement of the absolute branching fraction of the leptonic decay @xmath0 ( charge - conjugate modes are implied ) , with @xmath1 , from which we determine the decay constant @xmath33 . \\n we use a data sample of @xmath38 events provided by the cornell electron storage ring ( cesr ) and collected by the cleo - c detector at the center - of - mass ( cm ) energy @xmath39 mev , near @xmath3 peak production  @xcite . \\n the data sample consists of an integrated luminosity of @xmath40 @xmath41 containing @xmath42 @xmath3 pairs . \\n we have previously reported  @xcite measurements of @xmath43 and @xmath0 with a subsample of these data . a companion article  @xcite reports measurements of @xmath33 from @xmath43 and @xmath0 , with @xmath44 , using essentially the same data sample as the one used in this measurement . \\n the cleo - c detector  @xcite is a general - purpose solenoidal detector with four concentric components utilized in this measurement : a small - radius six - layer stereo wire drift chamber , a 47-layer main drift chamber , a ring - imaging cherenkov ( rich ) detector , and an electromagnetic calorimeter consisting of 7800 csi(tl ) crystals . \\n the two drift chambers operate in a @xmath45  t magnetic field and provide charged particle tracking in a solid angle of @xmath46% of @xmath47 . \\n the chambers achieve a momentum resolution of @xmath48% at @xmath49  gev/@xmath50 . \\n the main drift chamber also provides specific - ionization ( @xmath51 ) measurements that discriminate between charged pions and kaons . \\n the rich detector covers approximately @xmath52% of @xmath47 and provides additional separation of pions and kaons at high momentum . \\n the photon energy resolution of the calorimeter is @xmath53% at @xmath54  gev and @xmath55% at @xmath56  mev . \\n electron identification is based on a likelihood variable that combines the information from the rich detector , @xmath51 , and the ratio of electromagnetic shower energy to track momentum ( @xmath57 ) . \\n we use a geant - based  @xcite monte carlo ( mc ) simulation program to study efficiency of signal - event selection and background processes . \\n physics events are generated by evtgen  @xcite , tuned with much improved knowledge of charm decays  @xcite , and final - state radiation ( fsr ) is modeled by the photos  @xcite program . \\n the modeling of initial - state radiation ( isr ) is based on cross sections for @xmath3 production at lower energies obtained from the cleo - c energy scan  @xcite near the cm energy where we collect the sample . \\n the presence of two @xmath58 mesons in a @xmath3 event allows us to define a single - tag ( st ) sample in which a @xmath58 is reconstructed in a hadronic decay mode and a further double - tagged ( dt ) subsample in which an additional @xmath59 is required as a signature of @xmath60 decay , the @xmath59 being the daughter of the @xmath60 . \\n the @xmath61 reconstructed in the st sample can be either primary or secondary from @xmath62 ( or @xmath63 ) . \\n the st yield can be expressed as @xmath64 where @xmath65 is the produced number of @xmath3 pairs , @xmath66 is the branching fraction of hadronic modes used in the st sample , and @xmath67 is the st efficiency . \\n the @xmath68 counts the candidates , not events , and the factor of 2 comes from the sum of @xmath28 and @xmath61 tags . \\n our double - tag ( dt ) sample is formed from events with only a single charged track , identified as an @xmath69 , in addition to a st . \\n the yield can be expressed as @xmath70 where @xmath71 is the leptonic decay branching fraction , including the subbranching fraction of @xmath1 decay , @xmath72 is the efficiency of finding the st and the leptonic decay in the same event . from the st and dt yields we can obtain an absolute branching fraction of the leptonic decay @xmath71 , without needing to know the integrated luminosity or the produced number of @xmath3 pairs , @xmath73 where @xmath74 ( @xmath75 ) is the effective signal efficiency . because of the large solid angle acceptance with high segmentation of the cleo - c detector and the low multiplicity of the events with which we are concerned , @xmath76 , where @xmath77 is the leptonic decay efficiency . \\n hence , the ratio @xmath78 is insensitive to most systematic effects associated with the st , and the signal branching fraction @xmath71 obtained using this procedure is nearly independent of the efficiency of the tagging mode .      to minimize systematic uncertainties , we tag using three two - body hadronic decay modes with only charged particles in the final state . \\n the three st modes and @xmath79 are shorthand labels for @xmath80 events within mass windows ( described below ) of the @xmath81 peak in @xmath82 and the @xmath83 peak in @xmath84 , respectively . \\n no attempt is made to separate these resonance components in the @xmath85 dalitz plot . ] \\n are @xmath86 , @xmath79 , and @xmath87 . \\n using these tag modes also helps to reduce the tag bias which would be caused by the correlation between the tag side and the signal side reconstruction if tag modes with high multiplicity and large background were used . \\n the effect of the tag bias @xmath88 can be expressed in terms of the signal efficiency @xmath74 defined by @xmath89 where @xmath90 is the st efficiency when the recoiling system is the signal leptonic decay with single @xmath59 in the other side of the tag . \\n as the general st efficiency @xmath67 , when the recoiling system is any possible @xmath91 decays , will be lower than the @xmath90 , sizable tag bias could be introduced if the multiplicity of the tag mode were high , or the tag mode were to include neutral particles in the final state . as shown in sec . \\n [ sec : results ] , this effect is negligible in our chosen clean tag modes . \\n the @xmath92 decay is reconstructed by combining oppositely charged tracks that originate from a common vertex and that have an invariant mass within @xmath93 mev of the nominal mass  @xcite . \\n we require the resonance decay to satisfy the following mass windows around the nominal masses  @xcite : @xmath94 ( @xmath95 mev ) and @xmath96 ( @xmath97 mev ) . \\n we require the momenta of charged particles to be @xmath56 mev or greater to suppress the slow pion background from @xmath98 decays ( through @xmath99 ) . \\n we identify a st by using the invariant mass of the tag @xmath100 and recoil mass against the tag @xmath101 . \\n the recoil mass is defined as @xmath102 where @xmath103 is the net four - momentum of the @xmath4 beam , taking the finite beam crossing angle into account ; @xmath104 is the four - momentum of the tag , with @xmath105 computed from @xmath106 and the nominal mass  @xcite of the @xmath91 meson . \\n we require the recoil mass to be within @xmath107 mev of the @xmath108 mass  @xcite . \\n this loose window allows both primary and secondary @xmath91 tags to be selected .        to estimate the backgrounds in our st and dt yields from the wrong tag combinations ( incorrect combinations that , by chance , lie within the @xmath109 signal region ) , we use the tag invariant mass sidebands . \\n we define the signal region as @xmath110 mev @xmath111 mev , and the sideband regions as @xmath112 mev @xmath113 mev or @xmath114 mev @xmath115 mev , where @xmath116 is the difference between the tag mass and the nominal mass . \\n we fit the st @xmath109 distributions to the sum of double - gaussian signal function plus second - degree chebyshev polynomial background function to get the tag mass sideband scaling factor . \\n the invariant mass distributions of tag candidates for each tag mode are shown in fig . \\n [ fig : dm ] and the st yield and @xmath109 sideband scaling factor are summarized in table  [ table : data - single ] . \\n we find @xmath117 summed over the three tag modes . \\n .[table : data - single ] summary of single - tag ( st ) yields , where @xmath118 is the yield in the st mass signal region , @xmath119 is the yield in the sideband region , @xmath120 is the sideband scaling factor , and @xmath68 is the scaled sideband - subtracted yield . [ cols=\"<,>,>,>,>\",options=\"header \" , ]     we considered six semileptonic decays , @xmath121 @xmath122 , @xmath123 , @xmath124 , @xmath125 , @xmath126 , and @xmath127 , as the major sources of background in the @xmath128 signal region . \\n the second dominates the nonpeaking background , and the fourth ( with @xmath129 ) dominates the peaking background . \\n uncertainty in the signal yield due to nonpeaking background ( @xmath130 ) is assessed by varying the semileptonic decay branching fractions by the precision with which they are known  @xcite . \\n imperfect knowledge of @xmath131 gives rise to a systematic uncertainty in our estimate of the amount of peaking background in the signal region , which has an effect on our branching fraction measurement of @xmath132 . \\n we study differences in efficiency , data vs mc events , due to the extra energy requirement , extra track veto , and @xmath133 requirement , by using samples from data and mc events , in which _ both _ the @xmath134 and @xmath2 satisfy our tag requirements , i.e. , `` double - tag \\'\\' events . \\n we then apply each of the above - mentioned requirements and compare loss in efficiency of data vs mc events . in this way \\n we obtain a correction of @xmath135 for the extra energy requirement and systematic uncertainties on each of the three requirements of @xmath136 ( all equal , by chance ) . \\n the non-@xmath69 background in the signal @xmath69 candidate sample is negligible ( @xmath137 ) due to the low probability ( @xmath138 per track ) that hadrons ( @xmath139 or @xmath140 ) are misidentified as @xmath69  @xcite . \\n uncertainty in these backgrounds produces a @xmath141 uncertainty in the measurement of @xmath142 . \\n the secondary @xmath69 backgrounds from charge symmetric processes , such as @xmath143 dalitz decay ( @xmath144 ) and @xmath145 conversion ( @xmath146 ) , are assessed by measuring the wrong - sign signal electron in events with @xmath147 . the uncertainty in the measurement from this source \\n is estimated to be @xmath148 . \\n other possible sources of systematic uncertainty include @xmath68 ( @xmath137 ) , tag bias ( @xmath149 ) , tracking efficiency ( @xmath148 ) , @xmath59 identification efficiency ( @xmath150 ) , and fsr ( @xmath150 ) . combining all contributions in quadrature , \\n the total systematic uncertainty in the branching fraction measurement is estimated to be @xmath151 . \\n in summary , using the sample of @xmath152 tagged @xmath28 decays with the cleo - c detector we obtain the absolute branching fraction of the leptonic decay @xmath153 through @xmath154 @xmath155 where the first uncertainty is statistical and the second is systematic . \\n this result supersedes our previous measurement  @xcite of the same branching fraction , which used a subsample of data used in this work . \\n the decay constant @xmath33 can be computed using eq . \\n ( [ eq : f ] ) with known values  @xcite @xmath156 gev@xmath157 , @xmath158 mev , @xmath159 mev , and @xmath160 s. we assume @xmath161 and use the value @xmath162 given in ref . \\n we obtain @xmath163    combining with our other determination  @xcite of @xmath164 mev with @xmath43 and @xmath0 ( @xmath165 ) decays , we obtain @xmath166 this result is derived from absolute branching fractions only and is the most precise determination of the @xmath91 leptonic decay constant to date .    our combined result is larger than the recent lqcd calculation @xmath167 mev  @xcite by @xmath168 standard deviations . \\n the difference between data and lqcd for @xmath33 could be due to physics beyond the sm  @xcite , unlikely statistical fluctuations in the experimental measurements or the lqcd calculation , or systematic uncertainties that are not understood in the lqcd calculation or the experimental measurements .    combining with our other determination  @xcite of @xmath169 , via @xmath44 \\n , we obtain @xmath170 using this with our measurement  @xcite of @xmath171 , we obtain the branching fraction ratio @xmath172 this is consistent with @xmath173 , the value predicted by the sm with lepton universality , as given in eq . \\n ( [ eq : f ] ) with known masses  @xcite . \\n we gratefully acknowledge the effort of the cesr staff in providing us with excellent luminosity and running conditions . \\n d.  cronin - hennessy and a.  ryd thank the a.p .  sloan foundation . \\n this work was supported by the national science foundation , the u.s . \\n department of energy , the natural sciences and engineering research council of canada , and the u.k . \\n science and technology facilities council .        c.  amsler \\n _ et al . _ \\n ( particle data group ) , phys . \\n b * 667 * , 1 ( 2008 ) . \\n k.  ikado _ et al . _ \\n ( belle collaboration ) , phys . \\n lett .   * 97 * , 251802 ( 2006 ) . \\n b.  aubert _ et al . \\n _ ( babar collaboration ) , phys .  rev . \\n d * 77 * , 011107 ( 2008 ) . \\n a.  g.  akeroyd and c.  h.  chen , phys . \\n d * 75 * , 075004 ( 2007 ) ; a.  g.  akeroyd , prog . \\n phys .   * 111 * , 295 ( 2004 ) . \\n j.  l.  hewett , arxiv : hep - ph/9505246 . \\n w.  s.  hou , phys . \\n d * 48 * , 2342 ( 1993 ) . \\n e.  follana , c.  t.  h.  davies , g.  p.  lepage , and j.  shigemitsu ( hpqcd collaboration ) , phys . \\n lett .   * \\n 100 * , 062002 ( 2008 ) . \\n b.  i.  eisenstein _ et al . \\n _ ( cleo collaboration ) , phys .  rev . \\n d * 78 * , 052003 ( 2008 ) . \\n b.  a.  dobrescu and a.  s.  kronfeld , phys . \\n * 100 * , 241802 ( 2008 ) . \\n d.  cronin - hennessy _ et al . \\n _ ( cleo collaboration ) , arxiv:0801.3418 . \\n m.  artuso _ et al . _ \\n ( cleo collaboration ) , phys . \\n lett .   * 99 * , 071802 ( 2007 ) . \\n k.  m.  ecklund _ et al . _ \\n ( cleo collaboration ) , phys .  rev . \\n lett .   * 100 * , 161801 ( 2008 ) . \\n j.  p.  alexander _ et al . \\n _ ( cleo collaboration ) , phys .  rev . \\n d * 79 * , 052001 ( 2009 ) . \\n y.  kubota _ et al . \\n _ ( cleo collaboration ) , nucl . \\n instrum . \\n a * 320 * , 66 ( 1992 ) . \\n d.  peterson _ \\n et al . _ , \\n instrum . \\n methods phys . \\n , sec .  a * 478 * , 142 ( 2002 ) . m.  artuso _ et al . _ , \\n nucl .  instrum . \\n methods phys . \\n a * 502 * , 91 ( 2003 ) . \\n s.  dobbs _ et al . \\n _ ( cleo collaboration ) , phys .  rev . \\n d * 76 * , 112001 ( 2007 ) . \\n j.  p.  alexander _ et al . \\n _ ( cleo collaboration ) , phys .  rev . \\n lett .   * 100 * , 161804 ( 2008 ) . \\n e.  barberio and z.  was , comput . \\n . commun . * 79 * , 291 ( 1994 ) .', 'abstract': 'we have studied the leptonic decay @xmath0 , via the decay channel @xmath1 , using a sample of tagged @xmath2 decays collected near the @xmath3 peak production energy in @xmath4 collisions with the cleo - c detector . \\n we obtain @xmath5 and determine the decay constant @xmath6  mev , where the first uncertainties are statistical and the second are systematic .'}\n"
          ]
        }
      ],
      "source": [
        "#LOAD DATASET + MODEL\n",
        "dataset_name = \"ccdv/arxiv-summarization\" # \"ccdv/arxiv-summarization\"\n",
        "configs = get_dataset_config_names(dataset_name)\n",
        "print(configs)\n",
        "\n",
        "ds_builder = load_dataset_builder(dataset_name, \"document\")\n",
        "print(ds_builder.info.description)\n",
        "print(ds_builder.info.features)\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "print(dataset)\n",
        "print(dataset['train'][1])\n",
        "\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(100))\n",
        "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H34stdnju-_9"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/pegasus-pubmed\" # \"google/pegasus-arxiv\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)       \n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)#.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p00OM08r42kW"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ug83vYj4qYN",
        "outputId": "3a433eff-3281-4e68-f6ff-ddf017140ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1232: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 256 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,   122,   109,  ...,     0,     0,     0],\n",
            "        [    0,   109,   613,  ...,  3627,   135,     1],\n",
            "        [    0,   109,  9498,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    0,   145,  1037,  ...,   117,   146,     1],\n",
            "        [    0, 21312, 10135,  ...,   757, 46819,     1],\n",
            "        [    0,   115,   114,  ..., 10287,   114,     1]])\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "batch = tokenizer(small_eval_dataset[\"article\"][0:10], truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
        "summaries = model.generate(**batch)\n",
        "print(summaries)\n",
        "summaries_decoded = tokenizer.batch_decode(summaries, skip_special_tokens=True)\n",
        "print(len(summaries_decoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f86BLwXbTrSK",
        "outputId": "370b55a3-b954-4870-cfe1-9aa8c82fd293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "\n",
        "\n",
        "def get_num_words(sentence):\n",
        "  print(len(sentence.split()))\n",
        "  return len(sentence.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOq03xoF8TJ0"
      },
      "outputs": [],
      "source": [
        "data = [summaries_decoded, small_eval_dataset[\"article\"][0:10], small_eval_dataset[\"abstract\"][0:10]]\n",
        "output = pd.DataFrame(data)\n",
        "output.to_csv('out_arxiv_pegapubmed.csv', index = False)  \n",
        "!cp out_arxiv_pegapubmed.csv \"drive/My Drive/model output\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaf3TmYTuvQI",
        "outputId": "0d6eabb3-1bee-4f58-d868-56dcd3cb8bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.3297318362722128, recall=0.37571958692094853, fmeasure=0.34938808189621456), mid=Score(precision=0.41103223688442536, recall=0.4606072496724332, fmeasure=0.41290229119500926), high=Score(precision=0.504641645417791, recall=0.540496374250508, fmeasure=0.4982351256447917)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.10890657372020254, recall=0.12659358538750734, fmeasure=0.11288098380240152), mid=Score(precision=0.1703658650548369, recall=0.1896564356999355, fmeasure=0.17258321220086842), high=Score(precision=0.2565072796239066, recall=0.26755406390370445, fmeasure=0.2557864182984332)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.17715492159964683, recall=0.20074026109698137, fmeasure=0.1852854439133198), mid=Score(precision=0.22524844648420078, recall=0.2535185538385293, fmeasure=0.22770130403990696), high=Score(precision=0.2991746860887209, recall=0.3130637759100944, fmeasure=0.2895474793561775)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.24620942798708606, recall=0.281044003152308, fmeasure=0.25916881681243287), mid=Score(precision=0.32480213339071695, recall=0.35676686370492594, fmeasure=0.3246289580282199), high=Score(precision=0.42628596165222843, recall=0.4455780641095824, fmeasure=0.42020091855585473))}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"rouge\")\n",
        "metric.compute(predictions=summaries_decoded, references=small_eval_dataset[\"abstract\"][0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzqlaEJgtw_g"
      },
      "outputs": [],
      "source": [
        "print(hi)\n",
        "max_input_length = 3000\n",
        "max_target_length = 200\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    #inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(examples[\"article\"], max_length=max_input_length, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"abstract\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "#tokenized_data = dataset.map(preprocess_function, batched=True)\n",
        "#tokenized_data_val2 = dataset['validation'].map(preprocess_function, batched=True)\n",
        "#tokenized_dataset_train = small_train_dataset.map(preprocess_function, batched=True) #dataset['train']\n",
        "tokenized_dataset_val = small_eval_dataset.map(preprocess_function, batched=True) #dataset['validation']\n",
        "#tokenized_dataset_test = small_test_dataset.map(preprocess_function, batched=True) #dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFUbUnBn0ftP"
      },
      "outputs": [],
      "source": [
        "print(tokenized_dataset_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaCbhenbQ4pq"
      },
      "outputs": [],
      "source": [
        "print(tokenized_dataset_val[\"labels\"])\n",
        "#tokenized_dataset_val.rename_column(\"input_ids\", '2')\n",
        "import numpy as np\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "\n",
        "#tokenized_dataset_val = {int(k):v for k,v in tokenized_dataset_val.items()}\n",
        "summaries = model.generate(tokenized_dataset_val[\"labels\"])#['input_ids']) #torch.Tensor(tokenized_dataset_val[0:100][0]))\n",
        "print(summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2sDNVme_3ME"
      },
      "outputs": [],
      "source": [
        "print(summaries.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBQ4FzdiTMLZ"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm  # for our loading bar\n",
        "\n",
        "text_data = []\n",
        "batch_count = 0\n",
        "batches = []\n",
        "for sample in tqdm(dataset['validation']):\n",
        "    # remove newline characters from each sample as we need to use exclusively as seperators\n",
        "    sample = sample['article'].replace('\\n', '')\n",
        "    text_data.append(sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVXQSo93bHrx"
      },
      "outputs": [],
      "source": [
        "text = \"Replace me by any text you'd like.\"\n",
        "#inputs = tokenizer(text_data[0:10], return_tensors='pt', padding=True, truncation=True)\n",
        "prediction = model.generate(tokenized_dataset_val[0:100])#model.generate(tokenized_datasets[\"train\"][0:])\n",
        "prediction = tokenizer.batch_decode(prediction)\n",
        "print(prediction)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0EQTtPSGgxE"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8hv4O51NpBT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "batch_size = 16\n",
        "model_name = \"google/pegasus-pubmed\".split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-xsum\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVEpSWNovQkj"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    #compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "#training_args = Seq2SeqTrainingArguments(\n",
        "#    output_dir=\"./results\",\n",
        "#    evaluation_strategy=\"epoch\",\n",
        "#    learning_rate=2e-5,\n",
        "#    per_device_train_batch_size=1000,\n",
        "#    per_device_eval_batch_size=500,\n",
        "#    weight_decay=0.01,\n",
        "#    save_total_limit=3,\n",
        "#    num_train_epochs=1,\n",
        "#)\n",
        "\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "print(tokenized_datasets[\"train\"][0])\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTnlD_C8qFFo"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "model_name = \"google/pegasus-pubmed\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "#TOKENIZE\n",
        "def preprocess_function(examples):\n",
        "\n",
        "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"abstract\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxi-AhA_0HkD"
      },
      "outputs": [],
      "source": [
        "print(tokenized_dataset[\"train\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMS4QPGochM1"
      },
      "outputs": [],
      "source": [
        "#pegasus\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(samples):\n",
        "    return tokenizer(samples['article'].replace('\\n', ''), padding=\"max_length\", truncation=True)\n",
        "#def preprocess_function(examples):\n",
        "#    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "#    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "#    with tokenizer.as_target_tokenizer():\n",
        "#        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        " #   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        " #   return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "#tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "import numpy as np\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return rouge.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset#,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Z82PELdWDU"
      },
      "outputs": [],
      "source": [
        "#batch = tokenizer(text_data, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBRK-4GgH1yL"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\", tokenizer=\"google/pegasus-xsum\", framework=\"pt\")\n",
        "summarizer(text_data)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMnVlz8ooKa+IQxgmm9ZCF/",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95f4de6581c547fcb76cd7d03bcbd77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_076cbd31ccd04a62ba7d81a82621dbcb",
              "IPY_MODEL_94d9a6ac56fd4241b664b2131d0d7356",
              "IPY_MODEL_be53bb8e1c4d459b8830656aa26fa161"
            ],
            "layout": "IPY_MODEL_4735f42c2c514d1e8b5329a62a37ece0"
          }
        },
        "076cbd31ccd04a62ba7d81a82621dbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57911031e8f54befb043878f75df08e3",
            "placeholder": "​",
            "style": "IPY_MODEL_203ff9836aab49b3be16f71cf0a1d4b6",
            "value": ""
          }
        },
        "94d9a6ac56fd4241b664b2131d0d7356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea4c7194e0741f8afc34176635561b8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38b0ffa48bc54990b50cf2ac0eb8f748",
            "value": 0
          }
        },
        "be53bb8e1c4d459b8830656aa26fa161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7d1ff173684294af25f39034643afe",
            "placeholder": "​",
            "style": "IPY_MODEL_390e6a6e9013491bb1c6ae7f9f4b308c",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "4735f42c2c514d1e8b5329a62a37ece0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57911031e8f54befb043878f75df08e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "203ff9836aab49b3be16f71cf0a1d4b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cea4c7194e0741f8afc34176635561b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "38b0ffa48bc54990b50cf2ac0eb8f748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca7d1ff173684294af25f39034643afe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "390e6a6e9013491bb1c6ae7f9f4b308c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "678cd0d1d84d4669a0b949e6e8bb6eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b1a8a27cced46f195ad6f6516d0ebe7",
              "IPY_MODEL_0a89b639776b4d6285b5c5441c237e43",
              "IPY_MODEL_bed65a339fb043f799924551571a064e"
            ],
            "layout": "IPY_MODEL_5968322559a54300ac83a0ceb04e19c4"
          }
        },
        "6b1a8a27cced46f195ad6f6516d0ebe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e251d5a838d4ea3b3713068932984fd",
            "placeholder": "​",
            "style": "IPY_MODEL_ccce28a72b564c0db2f32c9dcdd7bdb2",
            "value": "100%"
          }
        },
        "0a89b639776b4d6285b5c5441c237e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9843462224e34aa686e26cc8eb170888",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebc03b1a0fd74171b068a1c7a8e7075e",
            "value": 3
          }
        },
        "bed65a339fb043f799924551571a064e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4e0d9b57d20450faf8ef9bef67de474",
            "placeholder": "​",
            "style": "IPY_MODEL_c112d26a472a4cb9a41b1681c050a33a",
            "value": " 3/3 [00:00&lt;00:00,  4.56it/s]"
          }
        },
        "5968322559a54300ac83a0ceb04e19c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e251d5a838d4ea3b3713068932984fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccce28a72b564c0db2f32c9dcdd7bdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9843462224e34aa686e26cc8eb170888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc03b1a0fd74171b068a1c7a8e7075e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4e0d9b57d20450faf8ef9bef67de474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c112d26a472a4cb9a41b1681c050a33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}